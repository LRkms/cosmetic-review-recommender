import pandas as pd
from konlpy.tag import Mecab
from collections import Counter
import re
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import numpy as np
import os

# ===============================
# ì„¤ì •
# ===============================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = BertTokenizer.from_pretrained("beomi/kcbert-base")
model = BertForSequenceClassification.from_pretrained("beomi/kcbert-base")
model.to(device)
mecab = Mecab('C:/mecab/share/mecab-ko-dic')

# ë¶ˆìš©ì–´
stop_words = ['í”¼ë¶€', 'ì œí’ˆ', 'ì‚¬ìš©', 'ì„ ë¬¼', 'ë‚¨ì', 'í¬ë¦¼', 'ì €ë…', 'ì•„ë¹ ', 'êµ¬ë§¤', 'ì •ë„', 'ì•„ì¹¨',
              'ë‚¨ì„±', 'ëŠë‚Œ', 'íš¨ê³¼', 'í† ë„ˆ', 'ì¶”ì²œ', 'ì´ê±°', 'ê°€ê²©', 'ë§ˆë¬´ë¦¬', 'í›„ê¸°', 'íƒ€ì…',
              'ì„¸ì¼', 'ìƒê°', 'ê¸°íš', 'ê³ ë¯¼', 'ì—¬ë¦„', 'í™”ì¥', 'ê²¨ìš¸', 'ë‚¨í¸', 'ë§Œì¡±', 'ì˜¬ì¸ì›', 'ìš©ëŸ‰', 'í•˜ë‚˜',
              'ìŠ¤í‚¨', 'ë§ˆìŒ', 'ì²˜ìŒ', 'ë¦¬ë·°', 'ë¡œì…˜', 'ì…ë‹ˆë‹¤', 'ìê·¹', 'ë³´ìŠµ', 'ìˆ˜ë¶„', 'ì œí˜•', 'ì¹œêµ¬', 'êµ¬ì…']

# ===============================
# í•¨ìˆ˜ ì •ì˜
# ===============================

def preprocess_text(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s,.!?]', '', text)
    for stop_word in stop_words:
        text = text.replace(stop_word, '')
    sentences = re.split('[.!?]+', text)
    return [s.strip() for s in sentences if s.strip()]

def extract_tokens(text):
    if not isinstance(text, str):
        return []
    return mecab.morphs(text)

def predict_sentiment(text):
    if not isinstance(text, str):
        return -1
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        pred = torch.argmax(probs, dim=1).cpu().numpy()[0]
    return pred

def generate_ngrams(tokens, n):
    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

# ===============================
# ë°ì´í„° ì²˜ë¦¬
# ===============================

df = pd.read_csv('./data/all_reviews.csv')
df['sentences'] = df['review'].apply(preprocess_text)
df = df.explode('sentences')
df['tokens'] = df['sentences'].apply(extract_tokens)
df['sentiment'] = df['sentences'].apply(predict_sentiment)
df['bigrams'] = df['tokens'].apply(lambda x: generate_ngrams(x, 2))
df['trigrams'] = df['tokens'].apply(lambda x: generate_ngrams(x, 3))

# ===============================
# ì¶œë ¥ í´ë” ìƒì„±
# ===============================

os.makedirs("outputs", exist_ok=True)
markdown_lines = []
csv_rows = []

# ===============================
# ì œí’ˆë³„ ë¶„ì„ (ì¹´í…Œê³ ë¦¬-ì œí’ˆ ê¸°ì¤€)
# ===============================

products = df[['category', 'product']].drop_duplicates()

for _, row in products.iterrows():
    category = row['category']
    product = row['product']
    product_df = df[(df['category'] == category) & (df['product'] == product)]

    all_tokens = [token for tokens in product_df['tokens'] for token in tokens if len(token) > 1]
    all_bigrams = [bigram for bigrams in product_df['bigrams'] for bigram in bigrams]
    all_trigrams = [trigram for trigrams in product_df['trigrams'] for trigram in trigrams]

    token_counts = Counter(all_tokens)
    bigram_counts = Counter(all_bigrams)
    trigram_counts = Counter(all_trigrams)

    top_keywords = [kw for kw in token_counts.most_common(20) if kw[0] not in ['ì–´ìš”', 'ì•„ìš”', 'ëŠ”ë°', 'ìœ¼ë¡œ', 'ìŠµë‹ˆë‹¤']][:5]
    top_bigrams = bigram_counts.most_common(5)
    top_trigrams = trigram_counts.most_common(5)

    keyword_sentiment = {}
    for keyword, _ in top_keywords:
        keyword_sentences = product_df[product_df['sentences'].str.contains(keyword, na=False)]
        if not keyword_sentences.empty:
            total = len(keyword_sentences)
            positive = len(keyword_sentences[keyword_sentences['sentiment'] == 1])
            keyword_sentiment[keyword] = f"{positive / total * 100:.0f}% ê¸ì •"

    frequent_phrases = top_bigrams + top_trigrams

    summary = {}
    for keyword in keyword_sentiment.keys():
        keyword_sentences = product_df[product_df['sentences'].str.contains(keyword, na=False)]
        pos_reviews = keyword_sentences[keyword_sentences['sentiment'] == 1]['sentences'].head(1).iloc[0] if len(keyword_sentences[keyword_sentences['sentiment'] == 1]) > 0 else "ì—†ìŒ"
        neg_reviews = keyword_sentences[keyword_sentences['sentiment'] == 0]['sentences'].head(1).iloc[0] if len(keyword_sentences[keyword_sentences['sentiment'] == 0]) > 0 else "ì—†ìŒ"
        summary[keyword] = (pos_reviews, neg_reviews)

    # ì¶œë ¥ ë¬¸ìì—´ êµ¬ì„±
    markdown_lines.append(f"\n\n## ğŸ“¦ {category} | {product}")
    markdown_lines.append(f"\n### ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ê¸ì • ë¹„ìœ¨):")
    for keyword, sentiment in keyword_sentiment.items():
        markdown_lines.append(f"- {keyword}: {sentiment}")

    markdown_lines.append(f"\n### ğŸ’¬ ìì£¼ ì–¸ê¸‰ëœ êµ¬ì ˆ (bi-gram/tri-gram):")
    for phrase, count in frequent_phrases:
        markdown_lines.append(f"- {phrase}: {count}íšŒ")

    markdown_lines.append(f"\n### ğŸ“ ë¦¬ë·° ìš”ì•½:")
    for keyword, (pos, neg) in summary.items():
        markdown_lines.append(f"- {keyword}: {pos} / {neg}")

    markdown_lines.append(f"\n### ğŸ—£ï¸ ìƒìœ„ ë¦¬ë·° ë¬¸ì¥ (ë¹ˆë„ ê¸°ì¤€):")
    top_sentences = product_df.groupby('sentences').size().sort_values(ascending=False).head(5).index
    for sentence in top_sentences:
        markdown_lines.append(f"- {sentence}")

    # CSV ì €ì¥ìš© row ì¶”ê°€
    for keyword, sentiment in keyword_sentiment.items():
        pos, neg = summary[keyword]
        csv_rows.append({
            "category": category,
            "product": product,
            "keyword": keyword,
            "positive_rate": sentiment,
            "positive_example": pos,
            "negative_example": neg
        })

# ===============================
# ê²°ê³¼ ì €ì¥
# ===============================

with open("outputs/summary.md", "w", encoding="utf-8") as f:
    f.write("\n".join(markdown_lines))

pd.DataFrame(csv_rows).to_csv("outputs/summary.csv", index=False, encoding="utf-8-sig")
