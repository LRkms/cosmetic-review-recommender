import pandas as pd
from konlpy.tag import Mecab
from collections import Counter
import re
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import numpy as np
import os

# ===============================
# ì„¤ì •
# ===============================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = BertTokenizer.from_pretrained("beomi/kcbert-base")
model = BertForSequenceClassification.from_pretrained("beomi/kcbert-base")
model.to(device)
mecab = Mecab('C:/mecab/share/mecab-ko-dic')

# ë¶ˆìš©ì–´ (ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥)
stop_words = ['í”¼ë¶€', 'ì œí’ˆ', 'ì‚¬ìš©', 'ì„ ë¬¼', 'ë‚¨ì', 'í¬ë¦¼', 'ì €ë…', 'ì•„ë¹ ', 'êµ¬ë§¤', 'ì •ë„', 'ì•„ì¹¨',
              'ë‚¨ì„±', 'ëŠë‚Œ', 'íš¨ê³¼', 'í† ë„ˆ', 'ì¶”ì²œ', 'ì´ê±°', 'ê°€ê²©', 'ë§ˆë¬´ë¦¬', 'í›„ê¸°', 'íƒ€ì…',
              'ì„¸ì¼', 'ìƒê°', 'ê¸°íš', 'ê³ ë¯¼', 'ì—¬ë¦„', 'í™”ì¥', 'ê²¨ìš¸', 'ë‚¨í¸', 'ë§Œì¡±', 'ì˜¬ì¸ì›', 'ìš©ëŸ‰', 'í•˜ë‚˜',
              'ìŠ¤í‚¨', 'ë§ˆìŒ', 'ì²˜ìŒ', 'ë¦¬ë·°', 'ë¡œì…˜', 'ì…ë‹ˆë‹¤', 'ìê·¹', 'ë³´ìŠµ', 'ìˆ˜ë¶„', 'ì œí˜•', 'ì¹œêµ¬', 'êµ¬ì…',
              'ê·¸ëƒ¥', 'ì™„ì „', 'ì§„ì§œ', 'ì •ë§', 'ì—„ì²­', 'ë„ˆë¬´', 'ì¢€', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì‚´ì§', 'ì§„ì§œë¡œ',
              'ì–´ìš”', 'ì•„ìš”', 'ëŠ”ë°', 'ìœ¼ë¡œ', 'ìŠµë‹ˆë‹¤', 'í•´ì„œ', 'ë¼ì„œ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ìœ¼ë‹ˆê¹Œ',
              'ë„¤ìš”', 'ì´ì—ìš”', 'ì˜ˆìš”', 'ì´ë¼', 'ë¼ê³ ', 'ë‹¤ê³ ', 'í•´ë„', 'ì¨ë„', 'í–ˆì–´ìš”', 'ê² ì–´ìš”']

# ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ íƒœê·¸
meaningful_pos = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'MAG', 'MAJ']


# ===============================
# í•¨ìˆ˜ ì •ì˜
# ===============================

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    if not isinstance(text, str):
        return []

    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^\w\s,.!?ã„±-ã…ã…-ã…£ê°€-í£]', ' ', text)

    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)

    # ë¬¸ì¥ ë¶„ë¦¬ (ë” ì •í™•í•œ íŒ¨í„´)
    sentences = re.split(r'[.!?]+\s*', text)

    # ë¹ˆ ë¬¸ì¥ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

    return sentences


def extract_meaningful_tokens(text):
    """ì˜ë¯¸ ìˆëŠ” í† í°ë§Œ ì¶”ì¶œ (í’ˆì‚¬ íƒœê¹… í™œìš©)"""
    if not isinstance(text, str) or len(text.strip()) < 2:
        return []

    try:
        # í’ˆì‚¬ íƒœê¹…
        pos_tags = mecab.pos(text)

        # ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ í•„í„°ë§
        meaningful_tokens = []
        for word, pos in pos_tags:
            # ê¸¸ì´ ì¡°ê±´ + í’ˆì‚¬ ì¡°ê±´ + ë¶ˆìš©ì–´ ì œì™¸
            if (len(word) >= 2 and
                    pos in meaningful_pos and
                    word not in stop_words and
                    not word.isdigit() and
                    not re.match(r'^[ã„±-ã…ã…-ã…£]+$', word)):  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²ƒ ì œì™¸
                meaningful_tokens.append(word)

        return meaningful_tokens
    except:
        return []


def predict_sentiment(text):
    """ê°ì • ë¶„ì„"""
    if not isinstance(text, str) or len(text.strip()) < 2:
        return -1

    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)
            pred = torch.argmax(probs, dim=1).cpu().numpy()[0]

        return pred
    except:
        return -1


def generate_meaningful_ngrams(tokens, n, min_count=2):
    """ì˜ë¯¸ ìˆëŠ” n-gram ìƒì„±"""
    if len(tokens) < n:
        return []

    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = ' '.join(tokens[i:i + n])
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ì¡°í•© ì œì™¸
        if len(ngram) >= 4:  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
            ngrams.append(ngram)

    return ngrams


def get_keyword_context(df, keyword, sentiment_filter=None):
    """í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
    keyword_sentences = df[df['sentences'].str.contains(keyword, na=False, regex=False)]

    if sentiment_filter is not None:
        keyword_sentences = keyword_sentences[keyword_sentences['sentiment'] == sentiment_filter]

    return keyword_sentences


# ===============================
# ë°ì´í„° ì²˜ë¦¬
# ===============================

print("ë°ì´í„° ë¡œë”© ì¤‘...")
df = pd.read_csv('./data/all_reviews.csv')

print("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
df['sentences'] = df['review'].apply(preprocess_text)
df = df.explode('sentences').reset_index(drop=True)

# ë¹ˆ ë¬¸ì¥ ì œê±°
df = df[df['sentences'].notna() & (df['sentences'] != '')].reset_index(drop=True)

print("í† í° ì¶”ì¶œ ì¤‘...")
df['tokens'] = df['sentences'].apply(extract_meaningful_tokens)

print("ê°ì • ë¶„ì„ ì¤‘...")
df['sentiment'] = df['sentences'].apply(predict_sentiment)

# ê°ì • ë¶„ì„ ì‹¤íŒ¨í•œ ê²ƒë“¤ ì œê±°
df = df[df['sentiment'] != -1].reset_index(drop=True)

print("N-gram ìƒì„± ì¤‘...")
df['bigrams'] = df['tokens'].apply(lambda x: generate_meaningful_ngrams(x, 2))
df['trigrams'] = df['tokens'].apply(lambda x: generate_meaningful_ngrams(x, 3))

# ===============================
# ì¶œë ¥ í´ë” ìƒì„±
# ===============================

os.makedirs("outputs", exist_ok=True)
markdown_lines = []
csv_rows = []

# ===============================
# ì œí’ˆë³„ ë¶„ì„ (ì¹´í…Œê³ ë¦¬-ì œí’ˆ ê¸°ì¤€)
# ===============================

print("ì œí’ˆë³„ ë¶„ì„ ì‹œì‘...")
products = df[['category', 'product']].drop_duplicates()

for idx, row in products.iterrows():
    category = row['category']
    product = row['product']

    print(f"ë¶„ì„ ì¤‘: {category} - {product}")

    product_df = df[(df['category'] == category) & (df['product'] == product)].copy()

    if len(product_df) < 5:  # ë„ˆë¬´ ì ì€ ë¦¬ë·°ëŠ” ê±´ë„ˆë›°ê¸°
        continue

    # í† í° ì§‘ê³„
    all_tokens = []
    for tokens in product_df['tokens']:
        all_tokens.extend(tokens)

    all_bigrams = []
    for bigrams in product_df['bigrams']:
        all_bigrams.extend(bigrams)

    all_trigrams = []
    for trigrams in product_df['trigrams']:
        all_trigrams.extend(trigrams)

    # ë¹ˆë„ ê³„ì‚°
    token_counts = Counter(all_tokens)
    bigram_counts = Counter(all_bigrams)
    trigram_counts = Counter(all_trigrams)

    # ìƒìœ„ í‚¤ì›Œë“œ ì„ ì • (ë¹ˆë„ê°€ ë†’ê³  ì˜ë¯¸ìˆëŠ” ê²ƒë“¤)
    top_keywords = []
    for word, count in token_counts.most_common(50):
        if count >= 3 and len(word) >= 2:  # ìµœì†Œ 3ë²ˆ ì´ìƒ ì–¸ê¸‰ë˜ê³  2ê¸€ì ì´ìƒ
            top_keywords.append((word, count))
        if len(top_keywords) >= 10:  # ìƒìœ„ 10ê°œê¹Œì§€
            break

    # í‚¤ì›Œë“œë³„ ê°ì • ë¶„ì„
    keyword_sentiment = {}
    for keyword, freq in top_keywords:
        keyword_sentences = get_keyword_context(product_df, keyword)

        if len(keyword_sentences) > 0:
            total = len(keyword_sentences)
            positive = len(keyword_sentences[keyword_sentences['sentiment'] == 1])
            negative = len(keyword_sentences[keyword_sentences['sentiment'] == 0])

            pos_rate = positive / total * 100
            keyword_sentiment[keyword] = {
                'positive_rate': pos_rate,
                'total_mentions': total,
                'positive_count': positive,
                'negative_count': negative
            }

    # ìƒìœ„ êµ¬ì ˆ (ë¹ˆë„ 2 ì´ìƒ)
    top_bigrams = [(phrase, count) for phrase, count in bigram_counts.most_common(10) if count >= 2]
    top_trigrams = [(phrase, count) for phrase, count in trigram_counts.most_common(10) if count >= 2]

    # í‚¤ì›Œë“œë³„ ëŒ€í‘œ ë¦¬ë·° ì¶”ì¶œ
    keyword_examples = {}
    for keyword in keyword_sentiment.keys():
        pos_sentences = get_keyword_context(product_df, keyword, sentiment_filter=1)
        neg_sentences = get_keyword_context(product_df, keyword, sentiment_filter=0)

        pos_example = pos_sentences['sentences'].iloc[0] if len(pos_sentences) > 0 else "ì—†ìŒ"
        neg_example = neg_sentences['sentences'].iloc[0] if len(neg_sentences) > 0 else "ì—†ìŒ"

        keyword_examples[keyword] = (pos_example, neg_example)

    # ë§ˆí¬ë‹¤ìš´ ì¶œë ¥ êµ¬ì„±
    markdown_lines.append(f"\n\n## ğŸ“¦ {category} | {product}")
    markdown_lines.append(f"*