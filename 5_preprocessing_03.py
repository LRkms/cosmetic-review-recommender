import pandas as pd
from konlpy.tag import Mecab
from collections import Counter
import re
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import numpy as np
import os

# ===============================
# ì„¤ì •
# ===============================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = BertTokenizer.from_pretrained("beomi/kcbert-base")
model = BertForSequenceClassification.from_pretrained("beomi/kcbert-base")
model.to(device)
model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

# Mecab ì´ˆê¸°í™” ê°œì„  (Windows/Linux í˜¸í™˜)
try:
    mecab = Mecab('C:/mecab/share/mecab-ko-dic')
except:
    try:
        mecab = Mecab()
    except:
        print("Mecab ì´ˆê¸°í™” ì‹¤íŒ¨. ì‹œìŠ¤í…œì— Mecabì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)

# ë¶ˆìš©ì–´ (ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥)
stop_words = ['í”¼ë¶€', 'ì œí’ˆ', 'ì‚¬ìš©', 'ì„ ë¬¼', 'ë‚¨ì', 'í¬ë¦¼', 'ì €ë…', 'ì•„ë¹ ', 'êµ¬ë§¤', 'ì •ë„', 'ì•„ì¹¨',
              'ë‚¨ì„±', 'ëŠë‚Œ', 'íš¨ê³¼', 'í† ë„ˆ', 'ì¶”ì²œ', 'ì´ê±°', 'ê°€ê²©', 'ë§ˆë¬´ë¦¬', 'í›„ê¸°', 'íƒ€ì…',
              'ì„¸ì¼', 'ìƒê°', 'ê¸°íš', 'ê³ ë¯¼', 'ì—¬ë¦„', 'í™”ì¥', 'ê²¨ìš¸', 'ë‚¨í¸', 'ë§Œì¡±', 'ì˜¬ì¸ì›', 'ìš©ëŸ‰', 'í•˜ë‚˜',
              'ìŠ¤í‚¨', 'ë§ˆìŒ', 'ì²˜ìŒ', 'ë¦¬ë·°', 'ë¡œì…˜', 'ì…ë‹ˆë‹¤', 'ìê·¹', 'ë³´ìŠµ', 'ìˆ˜ë¶„', 'ì œí˜•', 'ì¹œêµ¬', 'êµ¬ì…',
              'ê·¸ëƒ¥', 'ì™„ì „', 'ì§„ì§œ', 'ì •ë§', 'ì—„ì²­', 'ë„ˆë¬´', 'ì¢€', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì‚´ì§', 'ì§„ì§œë¡œ',
              'ì–´ìš”', 'ì•„ìš”', 'ëŠ”ë°', 'ìœ¼ë¡œ', 'ìŠµë‹ˆë‹¤', 'í•´ì„œ', 'ë¼ì„œ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ìœ¼ë‹ˆê¹Œ',
              'ë„¤ìš”', 'ì´ì—ìš”', 'ì˜ˆìš”', 'ì´ë¼', 'ë¼ê³ ', 'ë‹¤ê³ ', 'í•´ë„', 'ì¨ë„', 'í–ˆì–´ìš”', 'ê² ì–´ìš”']

# ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ íƒœê·¸
meaningful_pos = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'MAG', 'MAJ']


# ===============================
# í•¨ìˆ˜ ì •ì˜
# ===============================

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    if not isinstance(text, str) or pd.isna(text):
        return []

    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^\w\s,.!?ã„±-ã…ã…-ã…£ê°€-í£]', ' ', text)

    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)

    # ë¬¸ì¥ ë¶„ë¦¬ (ë” ì •í™•í•œ íŒ¨í„´)
    sentences = re.split(r'[.!?]+\s*', text)

    # ë¹ˆ ë¬¸ì¥ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

    return sentences


def extract_meaningful_tokens(text):
    """ì˜ë¯¸ ìˆëŠ” í† í°ë§Œ ì¶”ì¶œ (í’ˆì‚¬ íƒœê¹… í™œìš©)"""
    if not isinstance(text, str) or len(text.strip()) < 2:
        return []

    try:
        # í’ˆì‚¬ íƒœê¹…
        pos_tags = mecab.pos(text)

        # ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ í•„í„°ë§
        meaningful_tokens = []
        for word, pos in pos_tags:
            # ê¸¸ì´ ì¡°ê±´ + í’ˆì‚¬ ì¡°ê±´ + ë¶ˆìš©ì–´ ì œì™¸
            if (len(word) >= 2 and
                    pos in meaningful_pos and
                    word not in stop_words and
                    not word.isdigit() and
                    not re.match(r'^[ã„±-ã…ã…-ã…£]+$', word)):  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²ƒ ì œì™¸
                meaningful_tokens.append(word)

        return meaningful_tokens
    except Exception as e:
        print(f"í† í° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []


def predict_sentiment(text):
    """ê°ì • ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
    if not isinstance(text, str) or len(text.strip()) < 2:
        return -1

    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)
            pred = torch.argmax(probs, dim=1).cpu().numpy()[0]

        return int(pred)  # numpy intë¥¼ Python intë¡œ ë³€í™˜
    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return -1


def predict_sentiment_batch(texts, batch_size=32):
    """ë°°ì¹˜ ë‹¨ìœ„ ê°ì • ë¶„ì„ (ì„±ëŠ¥ ê°œì„ )"""
    results = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        try:
            inputs = tokenizer(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=128)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=1).cpu().numpy()
                results.extend([int(pred) for pred in preds])

        except Exception as e:
            print(f"ë°°ì¹˜ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            results.extend([-1] * len(batch_texts))

    return results


def generate_meaningful_ngrams(tokens, n, min_count=2):
    """ì˜ë¯¸ ìˆëŠ” n-gram ìƒì„±"""
    if len(tokens) < n:
        return []

    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = ' '.join(tokens[i:i + n])
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ì¡°í•© ì œì™¸
        if len(ngram) >= 4:  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
            ngrams.append(ngram)

    return ngrams


def get_keyword_context(df, keyword, sentiment_filter=None):
    """í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
    try:
        keyword_sentences = df[df['sentences'].str.contains(keyword, na=False, regex=False)]

        if sentiment_filter is not None:
            keyword_sentences = keyword_sentences[keyword_sentences['sentiment'] == sentiment_filter]

        return keyword_sentences
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()


# ===============================
# ë°ì´í„° ì²˜ë¦¬
# ===============================

print("ë°ì´í„° ë¡œë”© ì¤‘...")
try:
    df = pd.read_csv('./data/all_reviews.csv')
    print(f"ì´ {len(df)}ê°œì˜ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ")
except FileNotFoundError:
    print("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. './data/all_reviews.csv' ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

print("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
df['sentences'] = df['review'].apply(preprocess_text)
df = df.explode('sentences').reset_index(drop=True)

# ë¹ˆ ë¬¸ì¥ ì œê±°
df = df[df['sentences'].notna() & (df['sentences'] != '')].reset_index(drop=True)
print(f"ì „ì²˜ë¦¬ í›„ {len(df)}ê°œì˜ ë¬¸ì¥")

print("í† í° ì¶”ì¶œ ì¤‘...")
df['tokens'] = df['sentences'].apply(extract_meaningful_tokens)

print("ê°ì • ë¶„ì„ ì¤‘... (ë°°ì¹˜ ì²˜ë¦¬)")
# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ê°œì„ 
sentences_list = df['sentences'].tolist()
sentiments = predict_sentiment_batch(sentences_list)
df['sentiment'] = sentiments

# ê°ì • ë¶„ì„ ì‹¤íŒ¨í•œ ê²ƒë“¤ ì œê±°
df = df[df['sentiment'] != -1].reset_index(drop=True)
print(f"ê°ì • ë¶„ì„ í›„ {len(df)}ê°œì˜ ë¬¸ì¥ ë‚¨ìŒ")

print("N-gram ìƒì„± ì¤‘...")
df['bigrams'] = df['tokens'].apply(lambda x: generate_meaningful_ngrams(x, 2))
df['trigrams'] = df['tokens'].apply(lambda x: generate_meaningful_ngrams(x, 3))

# ===============================
# ì¶œë ¥ í´ë” ìƒì„±
# ===============================

os.makedirs("outputs", exist_ok=True)
markdown_lines = []
csv_rows = []

# ===============================
# ì œí’ˆë³„ ë¶„ì„ (ì¹´í…Œê³ ë¦¬-ì œí’ˆ ê¸°ì¤€)
# ===============================

print("ì œí’ˆë³„ ë¶„ì„ ì‹œì‘...")
products = df[['category', 'product']].drop_duplicates()
print(f"ì´ {len(products)}ê°œ ì œí’ˆ ë¶„ì„ ì˜ˆì •")

for idx, row in products.iterrows():
    category = row['category']
    product = row['product']

    print(f"ë¶„ì„ ì¤‘ ({idx + 1}/{len(products)}): {category} - {product}")

    product_df = df[(df['category'] == category) & (df['product'] == product)].copy()

    if len(product_df) < 5:  # ë„ˆë¬´ ì ì€ ë¦¬ë·°ëŠ” ê±´ë„ˆë›°ê¸°
        print(f"  -> ë¦¬ë·°ê°€ {len(product_df)}ê°œë¡œ ë„ˆë¬´ ì ì–´ ê±´ë„ˆë›°ê¸°")
        continue

    # í† í° ì§‘ê³„
    all_tokens = []
    for tokens in product_df['tokens']:
        if isinstance(tokens, list):
            all_tokens.extend(tokens)

    all_bigrams = []
    for bigrams in product_df['bigrams']:
        if isinstance(bigrams, list):
            all_bigrams.extend(bigrams)

    all_trigrams = []
    for trigrams in product_df['trigrams']:
        if isinstance(trigrams, list):
            all_trigrams.extend(trigrams)

    # ë¹ˆë„ ê³„ì‚°
    token_counts = Counter(all_tokens)
    bigram_counts = Counter(all_bigrams)
    trigram_counts = Counter(all_trigrams)

    # ìƒìœ„ í‚¤ì›Œë“œ ì„ ì • (ë¹ˆë„ê°€ ë†’ê³  ì˜ë¯¸ìˆëŠ” ê²ƒë“¤)
    top_keywords = []
    for word, count in token_counts.most_common(50):
        if count >= 3 and len(word) >= 2:  # ìµœì†Œ 3ë²ˆ ì´ìƒ ì–¸ê¸‰ë˜ê³  2ê¸€ì ì´ìƒ
            top_keywords.append((word, count))
        if len(top_keywords) >= 10:  # ìƒìœ„ 10ê°œê¹Œì§€
            break

    # í‚¤ì›Œë“œë³„ ê°ì • ë¶„ì„
    keyword_sentiment = {}
    for keyword, freq in top_keywords:
        keyword_sentences = get_keyword_context(product_df, keyword)

        if len(keyword_sentences) > 0:
            total = len(keyword_sentences)
            positive = len(keyword_sentences[keyword_sentences['sentiment'] == 1])
            negative = len(keyword_sentences[keyword_sentences['sentiment'] == 0])

            pos_rate = positive / total * 100 if total > 0 else 0
            keyword_sentiment[keyword] = {
                'positive_rate': pos_rate,
                'total_mentions': total,
                'positive_count': positive,
                'negative_count': negative
            }

    # ìƒìœ„ êµ¬ì ˆ (ë¹ˆë„ 2 ì´ìƒ)
    top_bigrams = [(phrase, count) for phrase, count in bigram_counts.most_common(10) if count >= 2]
    top_trigrams = [(phrase, count) for phrase, count in trigram_counts.most_common(10) if count >= 2]

    # í‚¤ì›Œë“œë³„ ëŒ€í‘œ ë¦¬ë·° ì¶”ì¶œ
    keyword_examples = {}
    for keyword in keyword_sentiment.keys():
        pos_sentences = get_keyword_context(product_df, keyword, sentiment_filter=1)
        neg_sentences = get_keyword_context(product_df, keyword, sentiment_filter=0)

        pos_example = pos_sentences['sentences'].iloc[0] if len(pos_sentences) > 0 else "ì—†ìŒ"
        neg_example = neg_sentences['sentences'].iloc[0] if len(neg_sentences) > 0 else "ì—†ìŒ"

        keyword_examples[keyword] = (pos_example, neg_example)

    # ë§ˆí¬ë‹¤ìš´ ì¶œë ¥ êµ¬ì„±
    markdown_lines.append(f"\n\n## ğŸ“¦ {category} | {product}")
    markdown_lines.append(f"*ì´ ë¦¬ë·° ìˆ˜: {len(product_df)}ê°œ*\n")

    # ì „ì²´ ê°ì • ë¶„í¬
    total_reviews = len(product_df)
    positive_reviews = len(product_df[product_df['sentiment'] == 1])
    negative_reviews = len(product_df[product_df['sentiment'] == 0])
    pos_percentage = (positive_reviews / total_reviews * 100) if total_reviews > 0 else 0

    markdown_lines.append(f"### ğŸ“Š ì „ì²´ ê°ì • ë¶„ì„")
    markdown_lines.append(f"- ê¸ì •: {positive_reviews}ê°œ ({pos_percentage:.1f}%)")
    markdown_lines.append(f"- ë¶€ì •: {negative_reviews}ê°œ ({100 - pos_percentage:.1f}%)")

    # í‚¤ì›Œë“œë³„ ê°ì • ë¶„ì„ ê²°ê³¼
    if keyword_sentiment:
        markdown_lines.append(f"\n### ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ê°ì • ë¶„ì„")
        for keyword, stats in sorted(keyword_sentiment.items(), key=lambda x: x[1]['total_mentions'], reverse=True):
            pos_rate = stats['positive_rate']
            total = stats['total_mentions']
            pos_count = stats['positive_count']
            neg_count = stats['negative_count']

            sentiment_icon = "ğŸ˜Š" if pos_rate >= 70 else "ğŸ˜" if pos_rate >= 30 else "ğŸ˜"

            markdown_lines.append(f"- **{keyword}** {sentiment_icon} (ì–¸ê¸‰ {total}íšŒ)")
            markdown_lines.append(f"  - ê¸ì •: {pos_count}íšŒ ({pos_rate:.1f}%)")
            markdown_lines.append(f"  - ë¶€ì •: {neg_count}íšŒ ({100 - pos_rate:.1f}%)")

            # ëŒ€í‘œ ì˜ˆì‹œ
            if keyword in keyword_examples:
                pos_ex, neg_ex = keyword_examples[keyword]
                if pos_ex != "ì—†ìŒ":
                    markdown_lines.append(f"  - ê¸ì • ì˜ˆì‹œ: \"{pos_ex[:50]}...\"")
                if neg_ex != "ì—†ìŒ":
                    markdown_lines.append(f"  - ë¶€ì • ì˜ˆì‹œ: \"{neg_ex[:50]}...\"")

    # ì£¼ìš” êµ¬ì ˆ
    if top_bigrams:
        markdown_lines.append(f"\n### ğŸ’¬ ìì£¼ ì–¸ê¸‰ë˜ëŠ” êµ¬ì ˆ")
        for phrase, count in top_bigrams[:5]:
            markdown_lines.append(f"- \"{phrase}\" ({count}íšŒ)")

    # CSV ë°ì´í„° ì¶”ê°€
    for keyword, stats in keyword_sentiment.items():
        csv_rows.append({
            'category': category,
            'product': product,
            'keyword': keyword,
            'total_mentions': stats['total_mentions'],
            'positive_count': stats['positive_count'],
            'negative_count': stats['negative_count'],
            'positive_rate': stats['positive_rate']
        })

# ===============================
# ê²°ê³¼ ì €ì¥
# ===============================

print("ê²°ê³¼ ì €ì¥ ì¤‘...")

# ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
markdown_content = "\n".join(markdown_lines)
with open("outputs/product_analysis.md", "w", encoding="utf-8") as f:
    f.write("# ğŸ›ï¸ ì œí’ˆë³„ ë¦¬ë·° ê°ì • ë¶„ì„ ê²°ê³¼\n")
    f.write(markdown_content)

# CSV íŒŒì¼ ì €ì¥
if csv_rows:
    csv_df = pd.DataFrame(csv_rows)
    csv_df.to_csv("outputs/keyword_sentiment_analysis.csv", index=False, encoding="utf-8-sig")

print("âœ… ë¶„ì„ ì™„ë£Œ!")
print(f"ğŸ“ ê²°ê³¼ íŒŒì¼:")
print(f"  - outputs/product_analysis.md")
print(f"  - outputs/keyword_sentiment_analysis.csv")